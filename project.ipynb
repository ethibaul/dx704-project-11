{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 11 Project\n",
        "\n",
        "In this project, you will develop and test prompts asking a language model to classify text from a home services query and match it to an appropriate category of home services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj61ZbNGimPQ"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 11 Materials](https://github.com/bu-cds-dx704/dx704-project-11).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkZMQEtsd9qU"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr92v0qI-CMV"
      },
      "source": [
        "## Part 1 : Design a Short Prompt\n",
        "\n",
        "The provided file \"queries.txt\" contains sample text from requests by homeowners by email or phone.\n",
        "These queries need to be classified as requesting an electrical, plumbing, or roofing or roofing services.\n",
        "The provided file has columns query_id, query, and target_category.\n",
        "Write a prompt template of 200 characters or less with parameter `query` for the homeowner query.\n",
        "Your prompt should be suitable to use with the Python code `prompt_template.format(query=query)`.\n",
        "Test your prompt with the model `gemini-2.0-flash` and suitable parsing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCU9bNnBfPLFHcaEdilYn8Ifjy_V6S1Tqc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6JWnRmSDATUg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing', 'electrical', 'roofing', 'plumbing']\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "prompt_template = (\n",
        "    'Classify the homeowner request: \"{query}\". '\n",
        "    'Respond with one word: electrical, plumbing, or roofing.'\n",
        ")\n",
        "\n",
        "with open(\"short-prompt.txt\", \"w\") as f:\n",
        "    f.write(prompt_template)\n",
        "\n",
        "def make_prompt(query: str) -> str:\n",
        "    return prompt_template.format(query=query)\n",
        "\n",
        "# load test data\n",
        "df = pd.read_csv(\"queries.txt\", sep=\"\\t\")\n",
        "\n",
        "preds = []\n",
        "for _, row in df.iterrows():\n",
        "    query = row[\"query\"]\n",
        "    prompt = make_prompt(query)\n",
        "\n",
        "    # The code below was a modification of my own code provided to me by chatGPT when trying to resolve a \"Resources Exhausted\" warning.\n",
        "    # See acknowledgements.txt for more information.\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        raw_text = response.text.strip()\n",
        "        pred = raw_text.split()[0].lower()\n",
        "\n",
        "        allowed = {\"electrical\", \"plumbing\", \"roofing\"}\n",
        "        if pred not in allowed:\n",
        "            if \"elect\" in raw_text.lower():\n",
        "                pred = \"electrical\"\n",
        "            elif \"plumb\" in raw_text.lower():\n",
        "                pred = \"plumbing\"\n",
        "            elif \"roof\" in raw_text.lower():\n",
        "                pred = \"roofing\"\n",
        "            else:\n",
        "                pred = \"electrical\"  # fallback\n",
        "        preds.append(pred)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        time.sleep(5)  # wait longer after a failure\n",
        "        continue\n",
        "\n",
        "    time.sleep(0.05)  # half-second between requests, upped by me to 3 seconds after 10 errors, still got 3 errors... Upping to 5 secs\n",
        "\n",
        "    # This was the code I originally had that was giving me the resource error (preserved for posterity)\n",
        "    # generate model response\n",
        "    #response = model.generate_content(prompt)\n",
        "\n",
        "    # parse and clean output\n",
        "    #raw_text = response.text.strip()\n",
        "    #pred = raw_text.split()[0].lower()\n",
        "\n",
        "    #allowed = {\"electrical\", \"plumbing\", \"roofing\"}\n",
        "    #if pred not in allowed:\n",
        "        #if \"elect\" in raw_text.lower():\n",
        "            #pred = \"electrical\"\n",
        "        #elif \"plumb\" in raw_text.lower():\n",
        "            #pred = \"plumbing\"\n",
        "        #elif \"roof\" in raw_text.lower():\n",
        "            #pred = \"roofing\"\n",
        "        #else:\n",
        "            #pred = \"electrical\"  # default fallback\n",
        "\n",
        "    #preds.append(pred)\n",
        "\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4icrosdHAVHt"
      },
      "source": [
        "Save your prompt template in a file \"short-prompt.txt\".\n",
        "Save the results of your prompt testing in \"short-output.tsv\" with columns `query_id` and `predicted_category`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KYszoHLMCwFo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved prompt (short-prompt.txt)\n",
            "Saved predictions (short-output.tsv)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "out_df = pd.DataFrame({\n",
        "    \"query_id\": df[\"query_id\"],\n",
        "    \"predicted_category\": preds\n",
        "})\n",
        "out_df.to_csv(\"short-output.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Saved prompt (short-prompt.txt)\")\n",
        "print(\"Saved predictions (short-output.tsv)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJs3OmYLFByW"
      },
      "source": [
        "Submit \"short-prompt.txt\" and \"short-output.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rErwnLaBFTD"
      },
      "source": [
        "Hint: your prompt may be re-tested with the Gemini API, so do not rely solely on lucky language model responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT6PZb-pDjyc"
      },
      "source": [
        "## Part 2: Find Short Prompt Mistakes\n",
        "\n",
        "Construct 5 queries of 100 characters or less that trick your short prompt so that the wrong category is chosen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "POBbyiXjE6vK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'query': 'Cabinet over 2nd floor bath is damp but all visible pipes are dry.', 'target_category': 'roofing', 'predicted_category': 'plumbing'}, {'query': 'Ceiling damp above hallway, only sometimes, bathroom nearby is dry.', 'target_category': 'roofing', 'predicted_category': 'plumbing'}, {'query': 'Toilet is fine but ceiling above it drips after long hot showers.', 'target_category': 'roofing', 'predicted_category': 'plumbing'}, {'query': 'Basement smells musty after outage; no visible leaks.', 'target_category': 'electrical', 'predicted_category': 'plumbing'}, {'query': 'My electric bill doubled after fixing a pipe leak last month.', 'target_category': 'plumbing', 'predicted_category': 'electrical'}]\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# same prompt\n",
        "prompt_template = (\n",
        "    'Classify the homeowner request: \"{query}\". '\n",
        "    'Respond with one word: electrical, plumbing, or roofing.'\n",
        ")\n",
        "\n",
        "def make_prompt(query: str) -> str:\n",
        "    return prompt_template.format(query=query)\n",
        "\n",
        "# 5 queries to hopefully confuse (by including ambiguous cross-referencing between categories)\n",
        "test_items = [\n",
        "    {\n",
        "        \"query\": \"Cabinet over 2nd floor bath is damp but all visible pipes are dry.\",\n",
        "        \"target_category\": \"roofing\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Ceiling damp above hallway, only sometimes, bathroom nearby is dry.\",\n",
        "        \"target_category\": \"roofing\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Toilet is fine but ceiling above it drips after long hot showers.\",\n",
        "        \"target_category\": \"roofing\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Basement smells musty after outage; no visible leaks.\",\n",
        "        \"target_category\": \"electrical\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"My electric bill doubled after fixing a pipe leak last month.\",\n",
        "        \"target_category\": \"plumbing\",\n",
        "    },\n",
        "]\n",
        "\n",
        "pred_rows = []\n",
        "\n",
        "for item in test_items:\n",
        "    q = item[\"query\"]\n",
        "    prompt = make_prompt(q)\n",
        "\n",
        "    # your original pattern\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        raw_text = response.text.strip()\n",
        "        pred = raw_text.split()[0].lower()\n",
        "\n",
        "        allowed = {\"electrical\", \"plumbing\", \"roofing\"}\n",
        "        if pred not in allowed:\n",
        "            low = raw_text.lower()\n",
        "            if \"elect\" in low:\n",
        "                pred = \"electrical\"\n",
        "            elif \"plumb\" in low:\n",
        "                pred = \"plumbing\"\n",
        "            elif \"roof\" in low:\n",
        "                pred = \"roofing\"\n",
        "            else:\n",
        "                pred = \"error\" \n",
        "    except Exception as e:\n",
        "        print(\"Error testing query:\", e)\n",
        "        pred = \"error\"  \n",
        "\n",
        "    pred_rows.append({\n",
        "        \"query\": q,\n",
        "        \"target_category\": item[\"target_category\"],\n",
        "        \"predicted_category\": pred\n",
        "    })\n",
        "\n",
        "    time.sleep(5)  \n",
        "\n",
        "print(pred_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BWsYeXTD_zm"
      },
      "source": [
        "Save your 5 queries in a file \"mistakes.tsv\" with columns `query`, `target_category` and `predicted_category`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QGizw9jiE_DW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# save to mistakes.tsv\n",
        "mistakes_df = pd.DataFrame(pred_rows)\n",
        "mistakes_df.to_csv(\"mistakes.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4keLgLXQE8X7"
      },
      "source": [
        "Submit \"mistakes.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjKBjMJnELDf"
      },
      "source": [
        "## Part 3: Design a Long Prompt\n",
        "\n",
        "Repeat part 1 with a length limit of 5000 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b_zluC6OEh92"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# define and save long prompt\n",
        "long_prompt_template = \"\"\"You are a professional home-services assistant.\n",
        "Your task is to classify a homeowner message into one of three service categories:\n",
        "1. Electrical - issues involving power, outlets, wiring, switches, lighting, breakers, electric panels, generators, or other electrical components.\n",
        "2. Plumbing - issues involving water flow, leaks, pipes, faucets, toilets, drains, hot-water heaters, dishwashers, sump pumps, or fixtures using water.\n",
        "3. Roofing - issues related to the roof, gutters, attic, ceilings leaking from above, shingles, storms, or water intrusion from rain or outside.\n",
        "\n",
        "If a request involves more than one type, choose the primary service needed.\n",
        "Read the homeowner query carefully, think briefly about the physical cause, then respond with exactly one word:\n",
        "electrical, plumbing, or roofing.\n",
        "\n",
        "Classify this homeowner request:\n",
        "\"{query}\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"long-prompt.txt\", \"w\") as f:\n",
        "    f.write(long_prompt_template)\n",
        "\n",
        "def make_long_prompt(query: str) -> str:\n",
        "    return long_prompt_template.format(query=query)\n",
        "\n",
        "# load the same input queries\n",
        "df = pd.read_csv(\"queries.txt\", sep=\"\\t\")\n",
        "\n",
        "preds = []\n",
        "for _, row in df.iterrows():\n",
        "    query = row[\"query\"]\n",
        "    prompt = make_long_prompt(query)\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        raw_text = response.text.strip()\n",
        "        pred = raw_text.split()[0].lower()\n",
        "\n",
        "        allowed = {\"electrical\", \"plumbing\", \"roofing\"}\n",
        "        if pred not in allowed:\n",
        "            low = raw_text.lower()\n",
        "            if \"elect\" in low:\n",
        "                pred = \"electrical\"\n",
        "            elif \"plumb\" in low:\n",
        "                pred = \"plumbing\"\n",
        "            elif \"roof\" in low:\n",
        "                pred = \"roofing\"\n",
        "            else:\n",
        "                pred = \"error\"  # default fallback\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        pred = \"error\"\n",
        "\n",
        "    preds.append(pred)\n",
        "    time.sleep(5)  # pacing as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing',\n",
              " 'electrical',\n",
              " 'roofing',\n",
              " 'plumbing']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfLQ0taZEjew"
      },
      "source": [
        "Save your longer prompt template in a file \"long-prompt.txt\".\n",
        "Save the results of your prompt testing in \"long-output.tsv\".\n",
        "Both files should use the same columns as part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7NjJQqD7E4Bv"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "out_df = pd.DataFrame({\n",
        "    \"query_id\": df[\"query_id\"],\n",
        "    \"predicted_category\": preds\n",
        "})\n",
        "out_df.to_csv(\"long-output.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWi8WbnnEwVh"
      },
      "source": [
        "Submit \"long-prompt.txt\" and \"long-output.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 4: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 5: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
